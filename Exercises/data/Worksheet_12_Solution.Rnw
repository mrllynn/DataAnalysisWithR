
\documentclass[a4paper,11pt]{report}
\input{../00_setup}
\rhead{\textbf{Solutions for Worksheet Week 12}}
\lfoot{Solution by Muriel Buri}
\rfoot{\texttt{muriellynnea.buri@uzh.ch}}

\usepackage{nicefrac}
\usepackage{amsmath}
\usepackage{MnSymbol,wasysym} % for Smileys

\begin{document}
\begin{description}

<<Preparation, eval=TRUE, echo=FALSE, message=FALSE>>=
rm(list = ls())
# Load libraries
library("knitr")
library("HSAUR2")
library("TH.data")
library("ggplot2")
# Define global chunk options
knitr::opts_chunk$set(eval=TRUE, echo=TRUE, fig.width=7, fig.height=3,
                      fig.show="asis", size="footnotesize")
## set working directory
setwd("~/Documents/Teaching/STA406/2016/exercises/12_20161206")
@

\vspace*{1mm}

\ex
Analyse the four randomised clinical trials on the prevention of
gastointestinal damages by Misoprostol whose data are given
in the \texttt{Lanza} data (contained in the
\textsf{R} add-on package \textbf{HSAUR2})
\begin{enumerate}[(a)]
\item Fit a joint cumulative logit model / proportional odds model to all four studies.
<<Exercise_1a, warning=FALSE, message=FALSE>>=
library("VGAM")
library("HSAUR2")
data("Lanza")
# str(Lanza)
# summary(Lanza)
### Define the Placebo group as the reference group.
Lanza$treatment <- factor(Lanza$treatment,
                          levels = c("Placebo", "Misoprostol"))
cum.log.mod.prop <- vglm(classification ~ treatment,
                         data = Lanza,
                         family=cumulative(parallel = TRUE))
# cum.log.mod.prop <- polr(classification ~ treatment,
#                          data = Lanza)
@

\item Fit a joint cumulative logit model without the proportional odds assumption
to all four studies.
<<Exercise_1b>>=
cum.log.mod <- vglm(classification ~ treatment,
                    data = Lanza,
                    family=cumulative(parallel = FALSE))
@

\item Is the proportional odds assumption reasonable in this case? \newline
We can understand the two models as nested models: The proportional odds model 
is nested in the cumulative logit model without the proportional odds assumption.
Given that, we can perform a likelihood ratio
test to test if the models are statistically different. \newline
The null hypothesis states that there is no difference in the coefficients
between the two models. Hence, we "hope" to get a non-significant test result.
<<Exercise_1c, fig.height=5>>=
pchisq(deviance(cum.log.mod.prop)-deviance(cum.log.mod),
       df=df.residual(cum.log.mod.prop)-df.residual(cum.log.mod), lower.tail=FALSE)
VGAM::lrtest(cum.log.mod.prop, cum.log.mod)
### --> The improvement in fit is statistically not significant.
# logLik(cum.log.mod.prop)
# logLik(cum.log.mod)
# AIC(cum.log.mod.prop)
# AIC(cum.log.mod)
# anova(cum.log.mod.prop, cum.log.mod, test="LRT")
# --> does NOT work for vlgm objects!
@
The $p$-value = $0.3241$ is quite high, - indicating that the proportional odds
model fits as well as the more complex joint cumulative logit model
\textbf{without} the proportional odds assumption.
<<Exercise_1c_1, fig.height=5>>=
(cf.cum.log.mod.prop <- coef(cum.log.mod.prop))
(cf.cum.log.mod <- coef(cum.log.mod))
ci.cum.log.mod <- confint(cum.log.mod)
res.cum.log.mod <- data.frame(parameter = names(cf.cum.log.mod),
                              number = gsub("[^0-9]", "", names(cf.cum.log.mod)),
                              estimate = cf.cum.log.mod,
                              lower = ci.cum.log.mod[,"2.5 %"],
                              upper = ci.cum.log.mod[,"97.5 %"])
ggplot(res.cum.log.mod, aes(y = estimate, x = parameter, colour = number)) +
  geom_point(size = I(4)) +
  geom_linerange(aes(ymin = lower, ymax = upper)) +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 320, hjust = 0))
@
The above plot also visualizes that the treatment effect of Misoprostol is
(more or less) the same for all classification groups. Hence, to fit a
proportional odds model is appropriate since we do not violate the proportional odds assumption.
% R for modeling dose-response data
% fit1 <- vglm(cbind(y1,y2,y3,y4,y5) ˜ dose,
%         family = cumulative(parallel = TRUE),
%         data = trauma)
% 
% R for modeling dose-response data without proportional odds
% fit2 <- vglm(cbind(y1,y2,y3,y4,y5) ˜ dose, family = cumulative, data = trauma)
% 
% R for modeling mental impairment data with partial proportional
% odds (life events but not SES)
% fit3 <- vglm(impair ˜ ses + life, family = cumulative(parallel = FALSE~ses))

\newpage

\ex
Fit a Box-Cox transformation model to the \texttt{bodyfat} data
(contained in the \textsf{R} add-on package \textbf{TH.data})
using the variable \texttt{DEXfat} as the response variable.
Use the \texttt{boxcox()} function from package \textbf{MASS}
to estimate the optimal transformation and the \texttt{lm()}
function for fitting the model. \newline
Compare the predictions to those of a normal linear model. \\
\textbf{HINT:} The optimal $\lambda$ can be found as follows
\vspace{-1em}
\begin{verbatim}
bc <- boxcox(...)
lambda <- bc$x[which.max(bc$y)]
\end{verbatim}
<<Exercise_2, fig.height=4.5>>=
library("MASS")
data("bodyfat", package = "TH.data")
# normal linear model
mod_lm <- lm(DEXfat ~ ., bodyfat)
# Find optimal lambda for Box-Cox-Transformation
mod_bc1 <- boxcox(mod_lm, lambda = seq(from = -2, to = 3, by = 0.1))
(lambda <- mod_bc1$x[which.max(mod_bc1$y)])
@
\begin{align*}
h(y) = \begin{cases}
\frac{y^{\lambda}-1}{\lambda} & \text{\textcolor{red}{$\lambda$}} \neq 0 \\
\log(y) & \text{\textcolor{red}{$\lambda$}} = 0
\end{cases}
\end{align*}
\textcolor{red}{\textbf{CAREFUL:}
\begin{itemize}
\item This Box-Cox-Transformation is depending on $\lambda$! (Sorry for my mistake!)
\item As Angelo stated correctly, the $0$ is in the CI (see plot above: $0 \in$ CI). \newline
However, $\lambda \neq 0$ $\Rightarrow$ $\frac{y^{\lambda}-1}{\lambda}$.
\end{itemize}}
<<Exercise_2_1>>=
## compute the transformation
bctrafo <- function(y, lambda) {
  if(lambda == 0) {
    return(log(y))
  } else {
    return((y^lambda - 1)/lambda)
  }
}
## compute the inverse transformation
invbctrafo <- function(ytrafo, lambda) {
  if(lambda == 0) {
    return(exp(ytrafo))
  } else {
    return(
      ((ytrafo * lambda) + 1)^(1/lambda)
    )
  }
}

### Compute model with Box-Cox-Transformation
bodyfat$ytrafo <- bctrafo(y = bodyfat$DEXfat, lambda = lambda)
mod_bc <- lm(ytrafo ~ ., bodyfat)

### Compare the predictions of the two models
pred_lm <- predict(mod_lm)
pred_bc <- invbctrafo(ytrafo = predict(mod_bc), lambda = lambda)

(mse_lm <- mean((bodyfat$DEXfat - pred_lm)^2))
(mse_bc <- mean((bodyfat$DEXfat - pred_bc)^2))
@
\end{enumerate}

\newpage

\ex
The data set \texttt{sambia.csv} given in Olat contains information on malnutrition
of sambian children. The outcome variable is the Z-score, which gives information
on the severity of malnutrition. The following covariates should be considered:

\begin{center}
\begin{tabular}{ll}
\hline
\texttt{zscore} & Z-score of the child\\
\texttt{k\_geschl} & Sex of the child (0 = female, 1 = male)\\
\texttt{k\_alter} & Age of the child\\
\texttt{m\_arbeit} & Is the mother working (0 = no, 1 = yes)\\
\texttt{m\_alterg} & Age of the mother at birth of child\\
\texttt{m\_bmi} & BMI of the mother\\
\hline
\end{tabular}
\end{center}

\begin{enumerate}[(a)]
\item Use Quantile regression (function \texttt{rq()} from package \textbf{quantreg}) to estimate 
$\beta_\tau$ once for $\tau = 0.025$ and once for $\tau = 0.5$. Compare the results. 
\item Estimate $\beta_\tau$ for a grid of values between $0.025$ and $0.5$. Plot the results 
using \texttt{plot.rqs()} and/or \texttt{plot.summary.rqs()}. What do the plots show?
\end{enumerate}

<<Exercise_3, message = FALSE, fig.height = 6>>=
library("quantreg")
sambia <- read.csv(file = "sambia.csv")
head(sambia)
### tau = 2.5 %
rq.025 <- rq(zscore ~ k_geschl + k_alter + m_arbeit + m_bmi + m_alterg, 
             tau = 0.025,
             data = sambia)
# summary(rq.025)

### tau = 5 %
rq.05 <- rq(zscore ~ k_geschl + k_alter + m_arbeit + m_bmi + m_alterg, 
            tau = 0.05, 
            data = sambia)
# summary(rq.05)

### tau = 50 %
rq.5 <- rq(zscore ~ k_geschl + k_alter + m_arbeit + m_bmi + m_alterg, 
           tau = 0.5, 
           data = sambia)
# summary(rq.5)


### Comparison
rq_coef <- cbind(coef(rq.025), coef(rq.05), coef(rq.5))
rownames(rq_coef) <- rownames(summary(rq.5)$coefficients)
colnames(rq_coef) <- paste0("tau = ", c("0.025", "0.05", "0.5"))
round(rq_coef, 3)

### Estimate for a grid of quantiles
rq <- rq(zscore ~ k_geschl + k_alter + m_arbeit + m_bmi + m_alterg, 
         tau = seq(from = 0.025, to = 0.5, by = 0.025), 
         data = sambia)

### Plot results
# lm(zscore ~ k_geschl + k_alter + m_arbeit + m_bmi + m_alterg, data = sambia)
plot(rq) # plot including line for the OLS coefficient (as estimated by lm)
@

\newpage

<<Exercise_3_1, message = FALSE, fig.height = 6>>=
# A sequence of coefficient estimates for quantile regressions with
# varying tau parameters is visualized along with associated confidence bands.
plot(summary(rq)) 
@

\end{description}
\end{document}